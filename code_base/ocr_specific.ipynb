{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.eval import EvalClient, EvalRunner\n",
    "from common_utils.image_utils import show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2030d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.buckets import CategoryEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5845f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.judge import JudgeClient, LLMJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666605ff",
   "metadata": {},
   "source": [
    "#### Generate Different Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalConfig:\n",
    "    base_url: str = \"http://localhost:8001/v1\"\n",
    "    api_key: str = \"EMPTY\"\n",
    "    # model: str = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    model: str = \"google/gemma-3-27b-it\"\n",
    "    temperature: float = 0.0\n",
    "    max_tokens: int = 128\n",
    "    \n",
    "eval_client = EvalConfig()\n",
    "\n",
    "@dataclass\n",
    "class JudgeConfig:\n",
    "    base_url: str = \"http://localhost:8000/v1\"\n",
    "    api_key: str = \"EMPTY\"\n",
    "    model: str = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    max_retries: int = 3\n",
    "    backoff: float = 0.6\n",
    "    max_tokens: int = 128\n",
    "\n",
    "judge_client = JudgeConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daaeb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77db06",
   "metadata": {},
   "source": [
    "### Test the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) dataset (your existing filtering)\n",
    "dataset = load_dataset(\"lmarena-ai/VisionArena-Chat\", split=\"train\", streaming=True)\n",
    "en_dataset = dataset.filter(lambda ex: ex[\"num_turns\"] == 1 and ex[\"language\"] == \"English\")\n",
    "small_ds = en_dataset.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c454e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) wire eval\n",
    "client = EvalClient(eval_client.base_url, eval_client.api_key)\n",
    "runner = EvalRunner(eval_client, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) choose categories & run 10 per\n",
    "categories = [\"ocr\"]  # add/remove as needed\n",
    "cat_runner = CategoryEvalRunner(runner, k=10, id_key=\"conversation_id\", exclusive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc69cf1",
   "metadata": {},
   "source": [
    "### Test one Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) pick one sample\n",
    "# it = iter(small_ds)\n",
    "# first = next(it)  # advance as you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e594245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_image(first.get(\"images\")[0]['bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed26e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4) run eval on the sample\n",
    "# out = runner.run_on_sample(first)\n",
    "# print(\"=== Eval Output ===\")\n",
    "# print(\"conv_id:\", out.conversation_id)\n",
    "# print(\"dataset_model:\", out.dataset_model)\n",
    "# print(\"user_question:\", out.user_question[:120], \"...\")\n",
    "# print(\"reference_answer:\", out.reference_answer[:120], \"...\")\n",
    "# print(\"model_answer:\", out.model_answer[:200], \"...\")\n",
    "# print(\"latency:\", out.latency_sec, \"sec\")\n",
    "# show_image(out.images[0].get('bytes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c29d6f",
   "metadata": {},
   "source": [
    "### Test one batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = cat_runner.build_buckets(iter(small_ds), categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934ca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36698ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) run eval on the sample\n",
    "out = runner.run_on_sample(buckets[\"ocr\"][0])\n",
    "print(\"=== Eval Output ===\")\n",
    "print(\"conv_id:\", out.conversation_id)\n",
    "print(\"dataset_model:\", out.dataset_model)\n",
    "print(\"user_question:\", out.user_question, \"...\")\n",
    "print(\"reference_answer:\", out.reference_answer, \"...\")\n",
    "print(\"model_answer:\", out.model_answer, \"...\")\n",
    "print(\"latency:\", out.latency_sec, \"sec\")\n",
    "show_image(out.images[0].get('bytes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(buckets['ocr'])):\n",
    "#     print(\"***************************************************************************************************\")\n",
    "#     # print(i)\n",
    "#     for k, v in buckets['ocr'][i].items():\n",
    "#         if k != \"images\":   # skip \"b\"\n",
    "#             print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_8803_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
