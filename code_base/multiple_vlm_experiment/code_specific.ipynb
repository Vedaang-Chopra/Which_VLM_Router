{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f70bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2406455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005de435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.eval import EvalClient, EvalRunner\n",
    "from common_utils.image_utils import show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85873cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.buckets import CategoryEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9be2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.judge import JudgeClient, LLMJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa6dc5",
   "metadata": {},
   "source": [
    "#### Generate Different Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73280e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalConfig:\n",
    "    base_url: str = \"http://localhost:8010/v1\"\n",
    "    api_key: str = \"EMPTY\"\n",
    "    model: str = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    # model: str = \"google/gemma-3-27b-it\"\n",
    "    # model: str = \"moonshotai/Kimi-VL-A3B-Thinking-2506\"\n",
    "    # model: str = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    temperature: float = 0.5\n",
    "    max_tokens: int = 2048\n",
    "    \n",
    "eval_client = EvalConfig()\n",
    "\n",
    "@dataclass\n",
    "class JudgeConfig:\n",
    "    base_url: str = \"http://localhost:8000/v1\"\n",
    "    api_key: str = \"EMPTY\"\n",
    "    model: str = \"google/gemma-3-27b-it\"    \n",
    "    max_retries: int = 3\n",
    "    backoff: float = 0.6\n",
    "    max_tokens: int = 2048\n",
    "    temperature: float = 0.0\n",
    "\n",
    "judge_client = JudgeConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace548aa",
   "metadata": {},
   "source": [
    "### Test the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7998133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) dataset (your existing filtering)\n",
    "dataset = load_dataset(\"lmarena-ai/VisionArena-Chat\", split=\"train\", streaming=True)\n",
    "en_dataset = dataset.filter(lambda ex: ex[\"num_turns\"] == 1 and ex[\"language\"] == \"English\")\n",
    "LEN_SMALL_DS = 2000\n",
    "small_ds = en_dataset.take(LEN_SMALL_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18aa5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) wire eval\n",
    "client = EvalClient(eval_client.base_url, eval_client.api_key)\n",
    "runner = EvalRunner(eval_client, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f47680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) choose categories & run 10 per\n",
    "categories = [\"ocr\", \"code\", \"is_code\", \"refusal\"]  # add/remove as needed\n",
    "cat_runner = CategoryEvalRunner(runner, k=10, id_key=\"conversation_id\", exclusive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846bdba",
   "metadata": {},
   "source": [
    "### Test one Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff05027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "k = random.randrange(LEN_SMALL_DS/10)\n",
    "sample = next(itertools.islice(small_ds, k, k+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aefb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) run eval on the sample\n",
    "out = runner.run_on_sample(sample)\n",
    "print(\"=== Eval Output ===\")\n",
    "print(\"conv_id:\", out.conversation_id)\n",
    "print(\"dataset_model:\", out.dataset_model)\n",
    "print(\"user_question:\", out.user_question[:120], \"...\")\n",
    "print(\"reference_answer:\", out.reference_answer[:120], \"...\")\n",
    "print(\"model_answer:\", out.model_answer[:200], \"...\")\n",
    "print(\"latency:\", out.latency_sec, \"sec\")\n",
    "show_image(out.images[0].get('bytes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295dc363",
   "metadata": {},
   "source": [
    "### Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8cbb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = cat_runner.build_buckets(iter(small_ds), categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374257a",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b14b69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_cat, all_results = cat_runner._run_buckets(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d631801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCollected {sum(len(v) for v in results_by_cat.values())} samples \"\n",
    "      f\"across {len(results_by_cat)} categories; flat array size: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5487d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: peek first item from the flat array\n",
    "if all_results:\n",
    "    r0 = all_results[1]\n",
    "    print(\"First result:\")\n",
    "    print(\" conv_id:\", r0.conversation_id)\n",
    "    print(\" category:\", next((c for c, rs in results_by_cat.items() if r0 in rs), None))\n",
    "    print(\" user_question:\", r0.user_question, \"...\")\n",
    "    print(\" model_answer:\", r0.model_answer, \"...\")\n",
    "    print(\" latency:\", r0.latency_sec)\n",
    "    show_image(r0.images[0].get('bytes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dcfeeb",
   "metadata": {},
   "source": [
    "#### Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0643170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f64bcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_eval_client = JudgeClient(judge_client.base_url, judge_client.api_key)\n",
    "judge = LLMJudge(judge_eval_client, judge_client.model, \n",
    "                 max_retries=judge_client.max_retries, backoff=judge_client.backoff, \n",
    "                 max_tokens=judge_client.max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8a36742",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb078f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.judge import CategoryJudgeRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faccb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) run judging over eval outputs\n",
    "judge_runner = CategoryJudgeRunner(judge, id_key=\"conversation_id\", verbose=True)\n",
    "judged_by_cat, judged_all = judge_runner.run(\n",
    "    results_by_cat=results_by_cat,\n",
    "    buckets=buckets,                      # from the bucketing step\n",
    "    judge_model_name=judge_client.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c82b19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) peek first\n",
    "if judged_all:\n",
    "    SAMPLE = 38\n",
    "    j0 = judged_all[SAMPLE]\n",
    "    print(\"\\nFirst judged:\")\n",
    "    print(\" conv_id:\", j0.conversation_id)\n",
    "    \n",
    "    print(\" user input:\", j0.user_question)\n",
    "    print(\" category:\", j0.category)\n",
    "    print(\" score:\", j0.judge_score)\n",
    "    print(\"orignal model:\", j0.dataset_model)\n",
    "    print(\" justification:\", j0.judge_justification)\n",
    "    print(\"***************************************************************************************************\")\n",
    "    print(\"reference answer:\", j0.reference_answer)\n",
    "    print(\"***************************************************************************************************\")\n",
    "    print()\n",
    "    print(\"model answer:\", j0.model_answer)\n",
    "    show_image(all_results[SAMPLE].images[0].get('bytes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abe7dc",
   "metadata": {},
   "source": [
    "#### Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7e28b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.results import ResultsWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a21a0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from results_writer import ResultsWriter\n",
    "writer = ResultsWriter(output_dir=\"outputs\", prefix=\"specific_qwen3b_eval_gemma27b_judge_new.json\", timestamp=False)\n",
    "\n",
    "# Save everything:\n",
    "# paths = writer.save_all(\n",
    "#     all_results=all_results,     # EvalOutput list\n",
    "#     judged_all=judged_all,       # JudgedOutput list\n",
    "#     buckets=buckets,             # raw selected records per category\n",
    "# )\n",
    "\n",
    "# print(\"Saved files:\")\n",
    "# for sect, files in paths.items():\n",
    "#     for kind, p in files.items():\n",
    "#         print(f\"  {sect:8} {kind:12} -> {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc2bab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_path = writer.save_one_json(\n",
    "    eval_by_cat=results_by_cat,\n",
    "    judged_by_cat=judged_by_cat,\n",
    "    stem=\"combined\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8794f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "py311_main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
